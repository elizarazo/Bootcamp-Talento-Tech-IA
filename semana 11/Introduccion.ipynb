{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"S6-MxktWyQrp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d25c2ad-9888-477b-959f-b29d51319027","executionInfo":{"status":"ok","timestamp":1724317458052,"user_tz":-120,"elapsed":33,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Acción seleccionada por el agente: izquierda\n"]}],"source":["#Aprendizaje por refuerzo\n","#Implementación básica de un agente RL:\n","\n","import random\n","\n","class AgenteRL:\n","    def __init__(self, acciones):\n","        self.acciones = acciones\n","\n","    def seleccionar_accion(self, estado):\n","        # Ejemplo de selección aleatoria de acción\n","        return random.choice(self.acciones)\n","\n","# Uso del agente RL\n","acciones_posibles = ['izquierda', 'derecha', 'arriba', 'abajo']\n","agente = AgenteRL(acciones_posibles)\n","estado_actual = [0,0] #Estado inicial del entorno\n","accion_seleccionada = agente.seleccionar_accion(estado_actual)\n","print(\"Acción seleccionada por el agente:\", accion_seleccionada)"]},{"cell_type":"code","source":["class EntornoRL:\n","    def __init__(self, estados):\n","        self.estados = estados\n","\n","    def tomar_accion(self,accion):\n","    #Simulación de la transición de estado\n","        nuevo_estado = random.choice(self.estados)\n","        recompensa = random.randint(-10,10)\n","        return nuevo_estado, recompensa\n","\n","#Uso del entorno RL\n","estados_posibles = ['A', 'B', 'C', 'D']\n","entorno = EntornoRL(estados_posibles)\n","accion = 'izquierda' #Acción seleccionada por el agente\n","nuevo_estado, recompensa = entorno.tomar_accion(accion)\n","print(\"Nuevo estado:\", nuevo_estado)\n","print(\"Recompensa recibida:\", recompensa)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OyFJwq60_w3","outputId":"e3dfdb2a-cfb6-4dfd-f7e2-dde92b036f5e","executionInfo":{"status":"ok","timestamp":1724317458054,"user_tz":-120,"elapsed":30,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Nuevo estado: A\n","Recompensa recibida: -10\n"]}]},{"cell_type":"code","source":["#Implementación de un algortimo de Q-Learning\n","\n","class QLearning:\n","    def __init__(self, estados, acciones, alpha=0.1, gamma=0.9,\n","        epsilon=0.1):\n","        self.estados = estados\n","        self.acciones = acciones\n","        self.alpha = alpha #Tasa de aprendizaje\n","        self.gamma = gamma #Factor de descuento\n","        self.epsilon = epsilon #Epsilon-greedy\n","        self.q_table = {}\n","\n","    def actualizar_q_table(self, estado_actual, accion, recompensa, nuevo_estado): #Actualiza la tabla Q\n","        if (estado_actual) not in self.q_table:\n","            self.q_table[(estado_actual)] = {a:0 for a in self.acciones}\n","        if (nuevo_estado) not in self.q_table:\n","            self.q_table[(nuevo_estado)] = {a:0 for a in self.acciones}\n","\n","        q_actual = self.q_table[(estado_actual)][accion]\n","        max_q_nuevo_estado = max(self.q_table[(nuevo_estado)].values())\n","        nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo_estado - q_actual)\n","        self.q_table[(estado_actual)][accion] = nuevo_q_valor"],"metadata":{"id":"_yTCjc642yAt","executionInfo":{"status":"ok","timestamp":1724317458054,"user_tz":-120,"elapsed":21,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#Uso del algoritmo Q-Learning\n","estados = ['A', 'B', 'C']\n","acciones = ['izquierda', 'derecha']\n","q_learning = QLearning(estados, acciones)\n","\n","#Simulación de una época de entrenamiento\n","estado_actual = 'A'\n","accion = 'izquierda'\n","nuevo_estado = 'B'\n","recompensa = 10\n","q_learning.actualizar_q_table(estado_actual, accion, recompensa, nuevo_estado)\n","\n","#Visualización de la tabla Q\n","print(\"Tabla Q actualizada\")\n","print(\"-------------------\")\n","for estado, acciones in q_learning.q_table.items():\n","    print(f\"{estado}: {acciones}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSLPudHS5P9i","outputId":"9698c89e-7ea5-426d-f6e6-acb31f8eb88a","executionInfo":{"status":"ok","timestamp":1724317458055,"user_tz":-120,"elapsed":21,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q actualizada\n","-------------------\n","A: {'izquierda': 1.0, 'derecha': 0}\n","B: {'izquierda': 0, 'derecha': 0}\n"]}]}]}