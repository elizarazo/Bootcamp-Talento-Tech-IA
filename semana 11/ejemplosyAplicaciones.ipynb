{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lU5tCge1BCo1","outputId":"5f48388b-506f-4821-d3c9-720c9a5b8d8b","executionInfo":{"status":"ok","timestamp":1724406825538,"user_tz":-120,"elapsed":3681,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-values después del entrenamiento:\n","[[[ 0.   -1.    0.   -1.  ]\n","  [ 0.   -1.8  -1.   -0.2 ]\n","  [ 0.   -1.16 -1.    1.  ]\n","  [ 0.    0.    0.    0.  ]]\n","\n"," [[-1.   -1.    0.   -1.8 ]\n","  [-1.   -1.8  -1.   -1.16]\n","  [-0.2  -1.8  -1.8  -0.2 ]\n","  [ 1.   -1.   -1.16  0.  ]]\n","\n"," [[-1.   -1.    0.   -1.8 ]\n","  [-1.8  -1.   -1.   -1.8 ]\n","  [-1.16 -1.   -1.8  -1.  ]\n","  [-0.2  -1.   -1.8   0.  ]]\n","\n"," [[-1.    0.    0.   -1.  ]\n","  [-1.8   0.   -1.   -1.  ]\n","  [-1.8   0.   -1.   -1.  ]\n","  [-1.    0.   -1.    0.  ]]]\n"]}],"source":["#Ejercicio 1: Implementación de Q-Learning en un entorno de gridworld simple\n","\n","import numpy as np\n","\n","#Definición del gridworld\n","gridworld = np.array([\n","    [-1, -1, -1, 1],\n","    [-1, -1, -1, -1],\n","    [-1, -1, -1, -1],\n","    [-1, -1, -1, -1],\n","])\n","\n","#Definición de las acciones posibles: arriba, abajo, izquierda, derecha\n","acciones = {\n","    'abajo': (-1, 0),\n","    'arriba': (1, 0),\n","    'izquierda': (0, -1),\n","    'derecha': (0, 1),\n","}\n","\n","# Mapeo de índices a acciones\n","indice_a_accion = list(acciones.keys())\n","\n","# Implementación de Q-Learning\n","Q = np.zeros((gridworld.shape[0], gridworld.shape[1], len(acciones)))\n","gamma = 0.8\n","alpha = 0.1\n","num_episodios = 1000\n","\n","for _ in range(num_episodios):\n","    estado = (0, 0)\n","    while estado != (0, 3):\n","        accion = np.random.choice(range(len(acciones)))\n","        nombre_accion = indice_a_accion[accion]\n","        nueva_fila = estado[0] + acciones[nombre_accion][0]\n","        nueva_col = estado[1] + acciones[nombre_accion][1]\n","\n","        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_col < gridworld.shape[1]:\n","            recompensa = gridworld[nueva_fila, nueva_col]\n","            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n","            Q[estado[0], estado[1], accion] = (1-alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n","            estado = (nueva_fila, nueva_col)\n","\n","print(\"Tabla Q-values después del entrenamiento:\")\n","print(Q)"]},{"cell_type":"code","source":["#Ejecicio 2: Aplicación del Aprendizaje por refuerzo en juegos\n","# Ejemplo: Implementación de Q-Learning para juego simple\n","\n","import numpy as np\n","\n","#Definición de las recompensas del juego (datos ficticios)\n","recompensa = {\n","    'ganar': 1,\n","    'perder': -1,\n","    'empatar': 0,\n","}\n","\n","#Definición de las acciones posibles: arriba, abajo, izquierda, derecha\n","acciones = {\n","    'abajo': (-1, 0),\n","    'arriba': (1, 0),\n","    'izquierda': (0, -1),\n","    'derecha': (0, 1),\n","}\n","\n","#Implementación de Q-Learning para el juego\n","Q={}\n","\n","def q_learning(estado_actual, accion, nuevo_estado, resultado):\n","    if (estado_actual, accion) not in Q:\n","        Q[(estado_actual, accion)] = np.zeros(len(acciones))\n","    if nuevo_estado not in Q:\n","       Q[nuevo_estado] = np.zeros(len(acciones))\n","       nuevo_valor = recompensa[resultado] + gamma * np.max(Q[nuevo_estado])\n","       Q[(estado_actual, accion)] = (1-alpha) * Q[(estado_actual, accion)] + alpha * nuevo_valor\n","\n","# Ejemplo de variables de entrada (deberían ser determinadas por la lógica del juego)\n","estado_actual = 'estado_inicial'\n","accion = 'abajo'\n","nuevo_estado = 'estado_siguiente'\n","resultado = 'ganar'  # Este valor podría ser 'ganar', 'perder', o 'empatar'\n","\n","# Llamada a la función Q-Learning con las variables de ejemplo\n","q_learning(estado_actual, accion, nuevo_estado, resultado)\n","\n","# Muestra la tabla Q después del entrenamiento\n","print(\"Tabla Q-values después del entrenamiento:\")\n","print(Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgWNYBUOFc6R","outputId":"240c3e41-9657-4b07-8b5d-37c4e887df06","executionInfo":{"status":"ok","timestamp":1724406825540,"user_tz":-120,"elapsed":25,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-values después del entrenamiento:\n","{('estado_inicial', 'abajo'): array([0.1, 0.1, 0.1, 0.1]), 'estado_siguiente': array([0., 0., 0., 0.])}\n"]}]},{"cell_type":"code","source":["#Ejercico 3 Aplicación del aprendizaje por refuerzo en robótica\n","\n","import numpy as np\n","\n","#Definición del entorno de navegación (datos ficticios)\n","\n","entorno = np.array([\n","        [0, 0, 0, 0, 0],\n","        [0,-1,-1,-1, 0],\n","        [0, 0,-1, 0, 0],\n","        [0,-1,-1,-1, 0],\n","        [0, 0, 0, 0, 0]\n","])\n","\n","# Definición de acciones posibles (arriba, abajo, izquierda, derecha)\n","acciones = [(0, -1), (0,1), (-1,0), (1,0)]\n","\n","#Implementaión de Q-Learning\n","Q = np.zeros ((entorno.shape[0], entorno.shape[1], len(acciones)))\n","\n","gamma = 0.9 # Factor de descuento\n","alpha = 0.1 #Tasa de aprendizaje\n","num_episodes = 100\n","\n","for _ in range(num_episodes):\n","    estado = (0,0)\n","    while True:\n","        accion = np.random.choice(range(len(acciones)))\n","        nueva_fila = estado[0] + acciones[accion][0]\n","        nueva_col = estado[1] + acciones [accion][1]\n","        if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_col < entorno.shape[1]:\n","            recompensa = entorno[nueva_fila, nueva_col]\n","            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n","            Q[estado[0], estado[1], accion] = (1-alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n","            estado = (nueva_fila, nueva_col)\n","            if recompensa == 1:\n","                break\n","            if recompensa == -1:\n","               break\n","\n","print(\"Valores Q después del entrenamiento:\")\n","print(Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPVUP4CcZmgP","executionInfo":{"status":"ok","timestamp":1724406826924,"user_tz":-120,"elapsed":1398,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}},"outputId":"0a0bfe67-f81a-4032-c24f-333bcd8be38f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Valores Q después del entrenamiento:\n","[[[ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.         -0.92023356]\n","  [ 0.          0.          0.         -0.81469798]\n","  [ 0.          0.          0.         -0.40951   ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -0.9835768   0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.271       0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.         -0.271      -0.271      -0.19      ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -0.3439      0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.          0.         -0.1         0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}]},{"cell_type":"code","source":["#Ejercicio 4: Aplicación del aprendizaje por refuerzo en gestión de recursos\n","\n","import numpy as np\n","\n","#Definición de los estados (niveles de inventario), acciones\n","#(órdenes de reabastecimiento) y recompensar (costos, ganancias, etc.)\n","\n","estados = ['Bajo', 'Medio', 'Alto']\n","acciones = ['Reabastecer', 'No reabastecer']\n","recompensas = {\n","    ('Bajo', 'Reabastecer'): 50,\n","    ('Bajo', 'No reabastecer'): -10,\n","    ('Medio', 'Reabastecer'): 30,\n","    ('Medio', 'No reabastecer'): 0,\n","    ('Alto', 'Reabastecer'): 10,\n","    ('Alto', 'No reabastecer'): -20,\n","}\n","\n","# Implementación de Q-Learning\n","Q = {}\n","gamma = 0.9 #Factor de descuento\n","alpha = 0.1 #Tasa de aprendizaje\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","    estado_actual = np.random.choice(estados)\n","    while True:\n","        accion = np.random.choice(acciones)\n","        recompensa = recompensas[(estado_actual, accion)]\n","        if (estado_actual) not in Q:\n","            Q[estado_actual] = {}\n","        if accion not in Q[estado_actual]:\n","            Q[estado_actual][accion] = 0\n","        nuevo_estado = np.random.choice(estados)\n","        max_nuevo_estado = max(Q[nuevo_estado].values()) if nuevo_estado in Q else 0\n","        Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n","        estado_actual = nuevo_estado\n","        if recompensa == 50 or recompensa ==30 or recompensa == 10:\n","           break\n","\n","print(\"Valores Q después del entrenamiento:\")\n","print(Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SnSWa9jCf5ou","executionInfo":{"status":"ok","timestamp":1724406826925,"user_tz":-120,"elapsed":15,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}},"outputId":"72ac343f-ad09-4f1d-d9f0-083d2b0d3fd5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Valores Q después del entrenamiento:\n","{'Bajo': {'Reabastecer': 307.7689198825961, 'No reabastecer': 250.8355513369802}, 'Alto': {'No reabastecer': 233.72031059733524, 'Reabastecer': 261.7943331081374}, 'Medio': {'Reabastecer': 286.5367604079317, 'No reabastecer': 262.0394856676396}}\n"]}]}]}