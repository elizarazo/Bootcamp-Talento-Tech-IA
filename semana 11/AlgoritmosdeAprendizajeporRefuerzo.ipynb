{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QQTFtzIzvfD","outputId":"ac986b20-5dcc-49aa-eaa9-68ca79b052a5","executionInfo":{"status":"ok","timestamp":1724406895681,"user_tz":-120,"elapsed":18,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Estados posibles: [0 1 2 3]\n","Acciones posibles: [0 1]\n","Recompensas por estado: {0: -1, 1: -1, 2: -1, 3: 10}\n"]}],"source":["#Ejercicio de Aprendizaje por Refuerzo en Python\n","\n","#Ejercicio 1: Introducción a los principales algoritmos de RL\n","#Define el entorno del juego\n","\n","import numpy as np\n","\n","class Environment:\n","    def __init__(self):\n","        self.state_space = np.array([0, 1, 2, 3]) #Estados posibles\n","        self.action_space = np.array([0, 1]) #Acciones posibles\n","        self.rewards = {0: -1, 1: -1, 2: -1, 3: 10} #Recompensas por estado\n","\n","#Crea una instancia del entorno\n","env = Environment()\n","\n","#Nuestra información del entorno\n","print(\"Estados posibles:\", env.state_space)\n","print(\"Acciones posibles:\", env.action_space)\n","print(\"Recompensas por estado:\", env.rewards)"]},{"cell_type":"code","source":["#Eejercicio: Q-Learning\n","import numpy as np\n","\n","# Inicializa la tabla Q con valores arbitrarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","#Define los parámetros del algortimo\n","alpha = 0.1 #Tasa de aprendizaje\n","gamma = 0.9 #Factor de descuento\n","\n","#Define los hiperparámetros del algoritmo\n","alpha = 0.1 #Tasa de aprendizaje\n","gamma = 0.9 #Factor de descuento\n","\n","# Entrena el agente utilizando Q-Learning\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space)  # Estado inicial\n","    while state != 3:  # Mientras no lleguemos al estado objetivo\n","        action = np.random.choice(env.action_space)  # Selecciona una acción aleatoria\n","        next_state = state + action  # Estado siguiente\n","\n","        # Verifica que el next_state esté en los límites válidos\n","        if next_state < 0 or next_state >= len(env.state_space):\n","            break  # Salir del bucle si el estado siguiente es inválido\n","\n","        # Verifica si next_state tiene una recompensa definida\n","        if next_state in env.rewards:\n","            reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n","        else:\n","            reward = 0  # Asigna una recompensa por defecto si no está definido\n","\n","        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","        state = next_state  # Actualiza el estado\n","\n","#Imprime la tabla Q final\n","print(\"Función Q-valor aprendida:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQLX-M-j1US9","outputId":"91302eab-ab2f-4ea3-f61f-13d4d6bcbadc","executionInfo":{"status":"ok","timestamp":1724406897585,"user_tz":-120,"elapsed":1915,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-valor aprendida:\n","[[ 4.58  6.2 ]\n"," [ 6.2   8.  ]\n"," [ 8.   10.  ]\n"," [ 0.    0.  ]]\n"]}]},{"cell_type":"code","source":["# Sarsa\n","\n","#Ejercicio 3:Sarsa\n","#Reinicializa la tabla Q con valore arbitrarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","#Entrena el agente utilizando Sarsa\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space) #Estado inicial\n","    action = np.random.choice(env.action_space) #Selecciona una acción aleatoria\n","    while state != 3: #Mientras no lleguemos al estado objetivo\n","        next_state = state + action #Estado siguiente\n","        next_action = np.random.choice(env.action_space) #Selecciona una acción aleatoria\n","        reward = env.rewards[next_state] #Recompensa por el estado siguiente\n","        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n","        state = next_state #Actualiza el estado\n","        action = next_action #Actualiza la acción\n","\n","#Imprime la tabla Q final que aprendió con Sara\n","print(\"Función Q-valor aprendida con Sarsa:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1r3ydsX66gg","outputId":"23fb99f1-7d1d-4c48-853b-942634adb000","executionInfo":{"status":"ok","timestamp":1724406899710,"user_tz":-120,"elapsed":2138,"user":{"displayName":"edgar lizarazo","userId":"04576770117662547959"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-valor aprendida con Sarsa:\n","[[ 1.2777783   3.45250935]\n"," [ 3.90311825  6.71750536]\n"," [ 6.31491751 10.        ]\n"," [ 0.          0.        ]]\n"]}]},{"cell_type":"code","source":["#Política con gradiente Monecarlo\n","#Ejercicio 4: Política de Gradiente de Montecarlo\n","# Incializa la política don probabilidades uniformes\n","\n","# Inicializa la política con probabilidades uniformes\n","policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n","\n","# Inicializa la tabla Q\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Define la función de recompensa promedio\n","def average_reward(Q):\n","    return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n","\n","# Entrena la política utilizando Gradiente de Montecarlo\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space)  # Estado inicial\n","    while state != 3:  # Mientras no lleguemos al estado objetivo\n","        action = np.random.choice(env.action_space, p=policy[state])  # Selecciona una acción basada en la política\n","        next_state = state + action  # Estado siguiente\n","\n","        # Verifica que el next_state esté dentro de los límites válidos\n","        if next_state < 0 or next_state >= len(env.state_space):\n","            break  # Salir del bucle si el estado siguiente es inválido\n","\n","        reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n","\n","        # Calcula el gradiente\n","        gradient = np.zeros_like(policy[state]) #Calcula el gradiente\n","        gradient[action] = 1\n","\n","        alpha = 0.01  # Tasa de aprendizaje\n","        policy[state] += alpha * gradient * (reward - average_reward(Q))\n","\n","        # Corrige cualquier valor negativo en la política\n","        policy[state] = np.maximum(policy[state], 0)\n","\n","        # Normaliza la política para que las probabilidades sumen 1\n","        policy[state] = policy[state] / np.sum(policy[state])\n","\n","        state = next_state  # Actualiza el estado\n","\n","#Muestra la política aprendida\n","print(\"Política aprendida por Gradiente de Montecarlo:\")\n","print(policy)"],"metadata":{"id":"TOwxQ0bL8Rkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Inicializa la política con valores uniformes (ya no son probabilidades)\n","policy = np.ones((len(env.state_space), len(env.action_space))) * 2  # Inicializa con valores mayores a 1\n","\n","# Inicializa la tabla Q\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Define la función de recompensa promedio\n","def average_reward(Q):\n","    return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n","\n","# Tasa de aprendizaje\n","alpha = 0.001  # Reduce la tasa de aprendizaje para actualizaciones suaves\n","\n","# Entrena la política utilizando Gradiente de Montecarlo\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space)  # Estado inicial\n","    while state != 3:  # Mientras no lleguemos al estado objetivo\n","        action = np.random.choice(env.action_space)  # Selecciona una acción basada en la política\n","        next_state = state + action  # Estado siguiente\n","\n","        # Verifica que el next_state esté dentro de los límites válidos\n","        if next_state < 0 or next_state >= len(env.state_space):\n","            break  # Salir del bucle si el estado siguiente es inválido\n","\n","        reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n","\n","        # Calcula el gradiente\n","        gradient = np.zeros_like(policy[state])\n","        gradient[action] = 1\n","\n","        # Actualiza la política sin normalizarla\n","        policy[state] += alpha * gradient * (reward - average_reward(Q))\n","\n","        state = next_state  # Actualiza el estado\n","\n","# Muestra la política aprendida\n","print(\"Política aprendida por Gradiente de Montecarlo:\")\n","print(policy)"],"metadata":{"id":"WxhTfmAtm5Yp"},"execution_count":null,"outputs":[]}]}